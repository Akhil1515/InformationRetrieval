The program was written in Python3.6

First import the required packages.
--> From nltk.tokenize import word_tokenize which helps to tokenize the words
--> Import re inorder to use regular expressions
--> Counter from collections is used to count hashable objects
--> From nltk.stem.wordnet import WordNetLemmatizer is used for lemmatization

The path to the cranfield location is entered as the first command line argument along with the python file "assignment3.py"
The path to the queries file containg queries is entered as the second command line argument along with the python file "assignment3.py"

In this program a simple statistical relevance model is built based on the vector relevance model, using the term-based index.


PROCESSING OF QUERIES:
----------------------
/people/cs/s/sanda/cs6322/hw3.queries contains 20 queries named as Q1,Q2,Q3,.....Q20

--> First these  queries are read and then all the digits, special characters and tags if present are removed from the queries.
--> Now they are tokenized by using nltk.word_tokenize() method and converted into lower case letters.
--> Stopwords are then removed by checking if the words are present in stopwords.words('english')
--> Then lemmatization is done for the tokens by using lemmatizer.lemmatize() method where lemmatizer is an object of WordNetLemmatizer().
--> During lemmatizing we are sending the tokens along with their parts_of_speech tag.

Constructing term based index:
-----------------------------
'/people/cs/s/sanda/cs6322/Cranfield/' contains 1400 cranfield files named as cranfield0001,cranfield0002,cranfield0003,......cranfield1400

--> First these 1400 files are read and all the sgml tags, punctuations are removed.
--> Next, they are tokenized using nltk.word_tokenize()
--> Tokens are then converted into lower case.
--> Stopwords are then removed by checking if the words are present in stopwords.words('english')
--> Then lemmatization is done for the tokens by using lemmatizer.lemmatize() method where lemmatizer is an object of WordNetLemmatizer().
--> During lemmatizing we are sending the tokens along with their parts_of_speech tag.

Term based index for lemmas will be in the following format:

TERM	DOCFREQ		[DOCID,TF,MAX_TF,DOCLEN]

where TERM stands the term
DOCFREQ stands for number of documents in which the term appeared
DOCID represents the document number eg.. 187,1210
TF indicates the number of times the term appeared in the document
MAX_TF is the frequency of the most frequent term or stem in that document
DOCLEN is the total number of word occurrences in the document

Sample output:
Term 	    	DF 	 [DocId, TF, MaxTF, DocLen]
ab:	    	1	[[744, 1, 4, 130]]
abbott:	    	1	[[1340, 1, 6, 118]]

abbreviated:	1	[[122, 1, 8, 233]]

ability:	3	[[51, 1, 9, 207], [77, 1, 10, 349], [738, 1, 7, 143]] 



COMPUTING WEIGHTS FOR DOCUMENTS:
-------------------------------
Weights W1 for all the documents are computed by using the formula:
w1 = round((0.4 + 0.6 * math.log10(tf + 0.5)/math.log10(maxtf + 1.0)),3)
weight1_fulldoc contains all the terms and their weights 

Weights W2 for all the documents are computed by using the formula:
w2 = round((0.4 + 0.6 * (tf / (tf + 0.5 + 1.5 * (doclen / avg_doclen1)))),3)
weight2_fulldoc contains all the terms and their weights


COMPUTING WEIGHTS FOR QUERIES:
-----------------------------
Weights W1 for all the queries are computed by using the formula:
w1 = round((0.4 + 0.6 * math.log10(tf + 0.5)/math.log10(maxtf_query + 1.0)) * (math.log10(coll_size / query_df) / math.log10(coll_size)),3)
weight1_fullquery contains all the terms and their weights

Weights W2 for all the queries are computed by using the formula:
w2 = round((0.4 + 0.6 * (tf / (tf + 0.5 + 1.5 * (query_doclen / avg_doclen))) * math.log10(coll_size / query_df) / math.log10(coll_size)),3)
weight2_fullquery contains all the terms and their weights


COMPUTING COSINE VALUES BY CALCULATING NORMALIZATIONS:
-----------------------------------------------------

For both queries and documents, w1 and w2 are normalized and cosine values are computed.
Hence we will rank the documents according to weighing schemes w1 & w2


OUTPUT
------
QUESTION 1. Turn in the vector representation of the query ,
and the top 5 documents ranked for the query under both weighting schemes .
You are also required to present the vector representations for each of the first 5 ranked documents.

Ans: Note: See solutions.txt  


QUESTION 2. Indicate the rank, score, external document identifier, and headline, for each of the top 5 documents for each query.

Ans: Note: See solutions.txt


QUESTION 3. Identify which documents you think are relevant and non-relevant for each query by inspecting the documents.

Ans:
Q1:
what similarity laws must be obeyed when constructing aeroelastic models
of heated high speed aircraft 
relevant Documents: 883
Non relevant documents: 591, 282, 180, 1026, 817

Q2:
what are the structural and aeroelastic problems associated with flight
of high speed aircraft 
relevant Documents: 876, 791
Non relevant documents: 1016, 775, 281

Q3:
what problems of heat conduction in composite slabs have been solved so
far 
relevant Documents: 1176, 23, 631
Non relevant documents: 1308, 842, 1011

Q4:
can a criterion be developed to show empirically the validity of flow
solutions for chemically reacting gas mixtures based on the simplifying
assumption of instantaneous local chemical equilibrium 
relevant Documents: 523
Non relevant documents: 181, 775

Q5:
what chemical kinetic system is applicable to hypersonic aerodynamic
problems 
relevant Documents: 224, 864
Non relevant documents: 183, 631, 1016

Q6:
what theoretical and experimental guides do we have as to turbulent
couette flow behaviour 
relevant Documents: 282, 842
Non relevant documents: 880, 320

Q7:
is it possible to relate the available pressure distributions for an
ogive forebody at zero angle of attack to the lower surface pressures of
an equivalent ogive forebody at angle of attack 
relevant Documents: 23, 746
Non relevant documents: 451, 864, 523

Q8:
what methods -dash exact or approximate -dash are presently available
for predicting body pressures at angle of attack
relevant Documents: 1399, 1176
Non relevant documents: 1360, 854, 1178

Q9:
papers on internal /slip flow/ heat transfer studies 
relevant Documents: 1358, 1083
Non relevant documents: 817, 282, 22

Q10:
are real-gas transport properties for air available over a wide range of
enthalpies and densities 
relevant Documents: None
Non relevant documents: 153, 880, 484, 1083

Q11:
is it possible to find an analytical,  similar solution of the strong
blast wave problem in the newtonian approximation 
relevant Documents: 862, 339
Non relevant documents: 223, 1102, 15

Q12:
how can the aerodynamic performance of channel flow ground effect
machines be calculated 
relevant Documents: 31, 875
Non relevant documents: 1177, 5, 746

Q13:
what is the basic mechanism of the transonic aileron buzz 
relevant Documents: 523, 281
Non relevant documents: 409, 139

Q14:
papers on shock-sound wave interaction 
relevant Documents: 21, 631
Non relevant documents: 283, 856

Q15:
material properties of photoelastic materials 
relevant Documents: 1401, 181
Non relevant documents: 224, 339

Q16:
can the transverse potential flow about a body of revolution be
calculated efficiently by an electronic computer 
relevant Documents: 102, 320
Non relevant documents: 878, 880, 1359

Q17:
can the three-dimensional problem of a transverse potential flow about
a body of revolution be reduced to a two-dimensional problem 
relevant Documents: 1291, 1103
Non relevant documents: 181, 1045, 1038

Q18:
are experimental pressure distributions on bodies of revolution at angle
of attack available 
relevant Documents: None
Non relevant documents: 880, 255, 817

Q19:
does there exist a good basic treatment of the dynamics of re-entry
combining consideration of realistic effects with relative simplicity of
results 
relevant Documents: 327, 1174
Non relevant documents: 251, 631

Q20:
has anyone formally determined the influence of joule heating,  produced
by the induced current,  in magnetohydrodynamic free convection flows
under general conditions 
relevant Documents: 1083, 409
Non relevant documents: 858, 182


QUESTION 4. Describe why the top-ranked non-relevant document for each query did not get a lower score.

Ans: The non-relevant documents with high rank got a high score because few terms in the query had relatively more weight. 
     These terms have very little importance in the matching relavance. 
     But due to high frequency, the document got higher score.

QUESTION 5. Briefly discuss the different effects you notice with the two weighting schemes,
 either on a query-by-query basis or overall, whichever is most illuminating. 
 For example, you can point out that the weighting scheme seems to be working for this query as well as a list of other queries, 
 but not for some other queries you have noticed. Try to explain why it works and why it does not work.

Ans: W1 uses Maximum Term Frequency:
	The weights and scores are dependent on the frequency of occurrence of the term.
        The actual meaning of the word is not considered here. 
	So we will get irrelevant answers if the word matches.

     W2 uses average document Length and Document Length
	Relevance factor is not considered in this case.
	The actual meaning of the word is not considered here.
	If we retrieve the results we get irrelevant documents as they do not contain the term. 
     


QUESTION 6. Describe the design decisions you made in building your ranking system.

Ans:
PROCESSING OF QUERIES:
----------------------
/people/cs/s/sanda/cs6322/hw3.queries contains 20 queries named as Q1,Q2,Q3,.....Q20

--> First these  queries are read and then all the digits, special characters and tags if present are removed from the queries.
--> Now they are tokenized by using nltk.word_tokenize() method and converted into lower case letters.
--> Stopwords are then removed by checking if the words are present in stopwords.words('english')
--> Then lemmatization is done for the tokens by using lemmatizer.lemmatize() method where lemmatizer is an object of WordNetLemmatizer().
--> During lemmatizing we are sending the tokens along with their parts_of_speech tag.

Constructing term based index:
-----------------------------
'/people/cs/s/sanda/cs6322/Cranfield/' contains 1400 cranfield files named as cranfield0001,cranfield0002,cranfield0003,......cranfield1400

--> First these 1400 files are read and all the sgml tags, punctuations are removed.
--> Next, they are tokenized using nltk.word_tokenize()
--> Tokens are then converted into lower case.
--> Stopwords are then removed by checking if the words are present in stopwords.words('english')
--> Then lemmatization is done for the tokens by using lemmatizer.lemmatize() method where lemmatizer is an object of WordNetLemmatizer().
--> During lemmatizing we are sending the tokens along with their parts_of_speech tag.

COMPUTING WEIGHTS FOR DOCUMENTS:
-------------------------------
Weights W1 for all the documents are computed by using the formula:
w1 = round((0.4 + 0.6 * math.log10(tf + 0.5)/math.log10(maxtf + 1.0)),3)
weight1_fulldoc contains all the terms and their weights 

Weights W2 for all the documents are computed by using the formula:
w2 = round((0.4 + 0.6 * (tf / (tf + 0.5 + 1.5 * (doclen / avg_doclen1)))),3)
weight2_fulldoc contains all the terms and their weights


COMPUTING WEIGHTS FOR QUERIES:
-----------------------------
Weights W1 for all the queries are computed by using the formula:
w1 = round((0.4 + 0.6 * math.log10(tf + 0.5)/math.log10(maxtf_query + 1.0)) * (math.log10(coll_size / query_df) / math.log10(coll_size)),3)
weight1_fullquery contains all the terms and their weights

Weights W2 for all the queries are computed by using the formula:
w2 = round((0.4 + 0.6 * (tf / (tf + 0.5 + 1.5 * (query_doclen / avg_doclen))) * math.log10(coll_size / query_df) / math.log10(coll_size)),3)
weight2_fullquery contains all the terms and their weights


COMPUTING COSINE VALUES BY CALCULATING NORMALIZATIONS:
-----------------------------------------------------

For both queries and documents, w1 and w2 are normalized and cosine values are computed.
Hence we will rank the documents according to weighing schemes w1 & w2











